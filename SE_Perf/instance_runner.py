#!/usr/bin/env python3
"""
æŒ‰å®ä¾‹è¿è¡Œçš„å…¥å£è„šæœ¬ï¼ˆæ§åˆ¶å…¨å±€å¹¶è¡Œåº¦ï¼‰ï¼Œå¹¶è°ƒç”¨ SE_Perf/perf_run.pyã€‚

ç›®æ ‡ï¼š
- å°†åŸæœ¬â€œä¸€ä¸ªè¿­ä»£ä¸­æ‰€æœ‰å®ä¾‹ä¸€èµ·è·‘â€çš„æ–¹å¼ï¼Œæ”¹ä¸ºâ€œä»¥å®ä¾‹ä¸ºå•ä½çš„ç‹¬ç«‹è¿è¡Œâ€ã€‚
- é€šè¿‡æœ¬è„šæœ¬ç»Ÿä¸€æ§åˆ¶å…¨å±€å¹¶è¡Œç¨‹åº¦ï¼ˆåŒæ—¶å¯åŠ¨å¤šå°‘ä¸ªå®ä¾‹çš„ perf_runï¼‰ã€‚
- å¯¹å®ä¾‹ç›®å½•è¿›è¡Œä¸´æ—¶é‡ç»„ï¼šå¤–å±‚ä¸ºå®ä¾‹åæ–‡ä»¶å¤¹ï¼Œå†…å±‚æ‰æ˜¯ JSONï¼ˆæ¯æ¬¡ä»…åŒ…å«ä¸€ä¸ª JSONï¼‰ï¼Œä»¥ä¾› perf_run ä»…å¤„ç†è¯¥å®ä¾‹ã€‚
- å¯¹è¾“å‡ºç›®å½•è¿›è¡Œæ”¹å†™ï¼šä½¿ç”¨ `output_dir/instance_name` ä½œä¸ºè¿è¡Œæ ¹ï¼Œæ–¹ä¾¿ä¸‹æ¸¸æŒ‰å®ä¾‹èšåˆã€‚

ç”¨æ³•ç¤ºä¾‹ï¼š
  python SE_Perf/instance_runner.py --config configs/se_perf_configs/deepseek-v3.1-Plan-AutoSelect.yaml \
         --max-parallel 2 --mode execute

å¯é€‰ï¼š
- ä½¿ç”¨ `--dry-run` ä»…é¢„è§ˆæœ¬è„šæœ¬å°†æ‰§è¡Œçš„å‘½ä»¤ï¼Œä¸å®é™…è°ƒç”¨ perf_runã€‚
- ä½¿ç”¨ `--limit` ä»…é€‰æ‹©å‰ N ä¸ªå®ä¾‹è¿›è¡Œå¿«é€Ÿè¯•è·‘ã€‚
"""

import argparse
import concurrent.futures
import contextlib
import os
import shutil
import signal
import subprocess
import sys
import threading
import time
from datetime import datetime
from pathlib import Path

import yaml
from core.utils.collect_outputs import collect_outputs

# ensure 'core' package is importable when running as script
sys.path.insert(0, str(Path(__file__).parent))


def _load_yaml(path: Path) -> dict:
    """è¯»å– YAML æ–‡ä»¶ä¸ºå­—å…¸ã€‚"""
    with open(path, encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def _ensure_abs(path_like: str | Path, base: Path) -> Path:
    """å°†è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ä»¥ base ä¸ºæ ¹ï¼‰ã€‚"""
    p = Path(path_like)
    return p if p.is_absolute() else (base / p)


def _discover_instances(instances_dir: Path) -> list[Path]:
    """æšä¸¾å®ä¾‹ JSON æ–‡ä»¶ã€‚"""
    return sorted(instances_dir.glob("*.json"))


def _apply_order_file(inst_files: list[Path], order_file: Path) -> list[Path]:
    """
    æ ¹æ® order_file æŒ‡å®šçš„é¡ºåºå¯¹ inst_files è¿›è¡Œè¿‡æ»¤å’Œæ’åºã€‚
    order_file æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªä»»åŠ¡åï¼ˆstemï¼‰æˆ–æ–‡ä»¶åï¼ˆnameï¼‰ã€‚
    """
    if not order_file.exists():
        print(f"Warning: Order file {order_file} not found. Ignoring.")
        return inst_files

    with open(order_file, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]

    # å»ºç«‹æŸ¥æ‰¾è¡¨
    name_to_path = {p.name: p for p in inst_files}
    stem_to_path = {p.stem: p for p in inst_files}

    ordered_files = []
    seen_paths = set()

    print(f"æ­£åœ¨æ ¹æ® {order_file} è°ƒæ•´ä»»åŠ¡é¡ºåº...")
    for line in lines:
        target = None
        # 1. å°è¯•æ–‡ä»¶åå…¨åŒ¹é…
        if line in name_to_path:
            target = name_to_path[line]
        # 2. å°è¯• stem åŒ¹é…
        elif line in stem_to_path:
            target = stem_to_path[line]
        # 3. å°è¯•è¡¥å…¨ .json ååŒ¹é…
        elif not line.lower().endswith(".json"):
            line_json = line + ".json"
            if line_json in name_to_path:
                target = name_to_path[line_json]

        if target:
            if target not in seen_paths:
                ordered_files.append(target)
                seen_paths.add(target)
        else:
            print(f"  Warning: Order file ä¸­çš„ä»»åŠ¡ '{line}' æœªåœ¨å®ä¾‹ç›®å½•ä¸­æ‰¾åˆ°ï¼Œè·³è¿‡ã€‚")

    print(f"  è°ƒæ•´å: {len(inst_files)} -> {len(ordered_files)} ä¸ªå®ä¾‹")
    return ordered_files


def _prepare_temp_space(inst_files: list[Path], tmp_root: Path) -> dict[str, Path]:
    """
    ç”Ÿæˆä¸´æ—¶ç©ºé—´ç»“æ„ï¼š
    - tmp_root/instance_name/instance_name.json

    è¿”å›ï¼š{instance_name: tmp_instance_dir}
    """
    tmp_root.mkdir(parents=True, exist_ok=True)
    mapping: dict[str, Path] = {}
    for fp in inst_files:
        name = fp.stem
        inst_dir = tmp_root / name
        inst_dir.mkdir(parents=True, exist_ok=True)
        target = inst_dir / f"{name}.json"
        shutil.copy(fp, target)
        mapping[name] = inst_dir
    return mapping


def _write_per_instance_config(
    base_cfg: dict,
    tmp_instance_dir: Path,
    instance_name: str,
    config_out_path: Path,
    timestamp: str,
    override_base_out: str | None = None,
) -> Path:
    """
    åŸºäºåŸå§‹ SE é…ç½®ç”Ÿæˆâ€œå•å®ä¾‹â€é…ç½®ï¼š
    - instances.instances_dir æŒ‡å‘ tmp_instance_dirï¼ˆä»…å«è¯¥å®ä¾‹ JSONï¼‰
    - output_dir åœ¨åŸæœ‰åŸºç¡€ä¸Šè¿½åŠ  "/instance_name"ï¼ˆperf_run å†…éƒ¨ä¼šå†æ‹¼æ¥è¿­ä»£ç›®å½•ï¼‰
    """
    cfg = dict(base_cfg)  # æµ…æ‹·è´è¶³å¤Ÿï¼ˆå­—æ®µå‡ä¸ºåŸç”Ÿç±»å‹æˆ–åµŒå¥—å­—å…¸ï¼‰

    instances = cfg.get("instances", {}) or {}
    instances["instances_dir"] = str(tmp_instance_dir)
    cfg["instances"] = instances

    orig_out = str(cfg.get("output_dir", "trajectories_perf/run_{timestamp}")).rstrip("/")
    base_out = str(override_base_out) if override_base_out else orig_out
    final_base_out = base_out.replace("{timestamp}", timestamp)
    cfg["output_dir"] = f"{final_base_out}/{instance_name}"

    # å¦‚æœé…ç½®äº† Global Memory ä¸” backend ä¸º chromaï¼Œä¸”æœªæ˜¾å¼æŒ‡å®š persist_path
    # åˆ™è‡ªåŠ¨å°† persist_path è®¾ç½®ä¸º {cfg["output_dir"]}/global_memory
    # è¿™æ ·æ¯ä¸ªå®ä¾‹çš„ ChromaDB å°±ä¼šå­˜å‚¨åœ¨å„è‡ªçš„è¾“å‡ºç›®å½•ä¸‹ï¼Œé¿å…å¹¶å‘å†™å…¥å†²çª
    global_memory_bank = cfg.get("global_memory_bank")
    if isinstance(global_memory_bank, dict) and global_memory_bank.get("enabled"):
        memory = global_memory_bank.get("memory", {})
        if memory.get("backend") == "chroma":
            chroma_cfg = memory.get("chroma", {})
            # å¼ºåˆ¶è¦†ç›– persist_path ä¸ºå®ä¾‹è¾“å‡ºç›®å½•ä¸‹çš„ global_memory
            chroma_cfg["persist_path"] = f"{final_base_out}/global_memory"
            memory["chroma"] = chroma_cfg
            global_memory_bank["memory"] = memory
            cfg["global_memory_bank"] = global_memory_bank

    config_out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(config_out_path, "w", encoding="utf-8") as f:
        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)
    return config_out_path


def _build_perf_cmd(config_path: Path, mode: str, project_root: Path) -> list[str]:
    """
    æ„å»ºè°ƒç”¨ perf_run.py çš„å‘½ä»¤ï¼ˆä¸æ‰§è¡Œï¼Œä»…è¿”å›å‘½ä»¤åˆ—è¡¨ï¼‰ã€‚
    """
    return [
        sys.executable,
        str(project_root / "SE_Perf" / "perf_run.py"),
        "--config",
        str(config_path),
        "--mode",
        mode,
    ]


import json


def _log_token_usage(output_dir: Path, logger=None):
    """
    ç»Ÿè®¡å¹¶è®°å½• Token ä½¿ç”¨æƒ…å†µ
    """
    token_log_file = output_dir / "token_usage.jsonl"
    if not token_log_file.exists():
        return

    total_prompt = 0
    total_completion = 0
    total = 0
    by_context: dict[str, dict[str, int]] = {}

    try:
        with open(token_log_file, encoding="utf-8") as f:
            for line in f:
                try:
                    rec = json.loads(line)
                    pt = int(rec.get("prompt_tokens") or 0)
                    ct = int(rec.get("completion_tokens") or 0)
                    tt = int(rec.get("total_tokens") or (pt + ct))
                    ctx = str(rec.get("context") or "unknown")

                    total_prompt += pt
                    total_completion += ct
                    total += tt

                    agg = by_context.setdefault(ctx, {"prompt": 0, "completion": 0, "total": 0})
                    agg["prompt"] += pt
                    agg["completion"] += ct
                    agg["total"] += tt
                except Exception:
                    continue

        print("\nğŸ“ˆ Token ä½¿ç”¨ç»Ÿè®¡:")
        print(f"  Total: {total} (Prompt: {total_prompt}, Completion: {total_completion})")
        if by_context:
            print("  æŒ‰ä¸Šä¸‹æ–‡åˆ†ç±»:")
            for ctx, vals in by_context.items():
                print(f"    - {ctx}: prompt={vals['prompt']}, completion={vals['completion']}, total={vals['total']}")

        # å¦‚æœæä¾›äº† loggerï¼Œåˆ™è®°å½•è¯¦ç»† JSON
        if logger:
            logger.info(
                json.dumps(
                    {
                        "token_usage_total": {"prompt": total_prompt, "completion": total_completion, "total": total},
                        "by_context": by_context,
                        "token_log_file": str(token_log_file),
                    },
                    ensure_ascii=False,
                )
            )
    except Exception:
        pass


def _aggregate_token_stats(instance_output_dirs: list[Path], base_root_dir: str):
    """
    èšåˆæ‰€æœ‰å®ä¾‹çš„ Token æ¶ˆè€—
    """
    total_prompt = 0
    total_completion = 0
    total = 0
    by_context: dict[str, dict[str, int]] = {}

    print("\n=== å…¨å±€ Token æ¶ˆè€—ç»Ÿè®¡ ===")

    for inst_dir in instance_output_dirs:
        token_file = inst_dir / "token_usage.jsonl"
        if not token_file.exists():
            continue

        try:
            with open(token_file, encoding="utf-8") as f:
                for line in f:
                    try:
                        rec = json.loads(line)
                        pt = int(rec.get("prompt_tokens") or 0)
                        ct = int(rec.get("completion_tokens") or 0)
                        tt = int(rec.get("total_tokens") or (pt + ct))
                        ctx = str(rec.get("context") or "unknown")

                        total_prompt += pt
                        total_completion += ct
                        total += tt

                        agg = by_context.setdefault(ctx, {"prompt": 0, "completion": 0, "total": 0})
                        agg["prompt"] += pt
                        agg["completion"] += ct
                        agg["total"] += tt
                    except Exception:
                        continue
        except Exception:
            pass

    print(f"ğŸ“ˆ Total All Instances: {total} (Prompt: {total_prompt}, Completion: {total_completion})")
    if by_context:
        print("  æŒ‰ä¸Šä¸‹æ–‡åˆ†ç±» (All Instances):")
        for ctx, vals in by_context.items():
            print(f"    - {ctx}: prompt={vals['prompt']}, completion={vals['completion']}, total={vals['total']}")

    # å°è¯•å†™å…¥æ€»çš„ token_usage.json åˆ°æ ¹ç›®å½•
    try:
        root_token_file = Path(base_root_dir) / "total_token_usage.json"
        with open(root_token_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "token_usage_total": {"prompt": total_prompt, "completion": total_completion, "total": total},
                    "by_context": by_context,
                },
                f,
                ensure_ascii=False,
                indent=2,
            )
        print(f"å·²ä¿å­˜å…¨å±€ Token ç»Ÿè®¡è‡³: {root_token_file}")
    except Exception as e:
        print(f"ä¿å­˜å…¨å±€ Token ç»Ÿè®¡å¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description="æŒ‰å®ä¾‹è¿è¡Œçš„å…¥å£è„šæœ¬ï¼ˆæ§åˆ¶å¹¶è¡Œåº¦ï¼‰ï¼Œå°è£…è°ƒç”¨ perf_run.py")
    parser.add_argument("--config", required=True, help="SE é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä½œä¸ºåŸºç¡€æ¨¡æ¿ï¼‰")
    parser.add_argument("--instances-dir", help="è¦†ç›–é…ç½®ä¸­çš„ instances_dirï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--max-parallel", type=int, default=1, help="åŒæ—¶å¹¶å‘çš„å®ä¾‹æ•°ï¼ˆå…¨å±€å¹¶è¡Œåº¦ï¼‰")
    parser.add_argument("--mode", choices=["demo", "execute"], default="execute", help="perf_run.py çš„è¿è¡Œæ¨¡å¼")
    parser.add_argument("--output-dir", help="è¦†ç›–é…ç½®ä¸­çš„ output_dirï¼ˆå¯é€‰ï¼Œç”¨äºç»­è·‘æˆ–æŒ‡å®šè¾“å‡ºæ ¹ç›®å½•ï¼‰")
    parser.add_argument("--limit", type=int, help="ä»…å¤„ç†å‰ N ä¸ªå®ä¾‹ï¼ˆå¿«é€Ÿè¯•è·‘ï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="ä»…é¢„è§ˆå‘½ä»¤ï¼Œä¸å®é™…æ‰§è¡Œ")
    parser.add_argument(
        "--order-file", help="æŒ‡å®šä»»åŠ¡æ‰§è¡Œé¡ºåºçš„æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªä»»åŠ¡å/æ–‡ä»¶åï¼‰ï¼Œè‹¥æŒ‡å®šåˆ™ä»…æ‰§è¡Œæ–‡ä»¶ä¸­çš„ä»»åŠ¡"
    )

    args = parser.parse_args()

    project_root = Path(__file__).resolve().parents[1]
    base_cfg_path = _ensure_abs(args.config, project_root)
    base_cfg = _load_yaml(base_cfg_path)

    # è§£æå®ä¾‹ç›®å½•ï¼ˆä¼˜å…ˆ CLI è¦†ç›–ï¼‰
    if args.instances_dir:
        inst_dir = _ensure_abs(args.instances_dir, project_root)
    else:
        inst_dir = _ensure_abs((base_cfg.get("instances", {}) or {}).get("instances_dir", "instances"), project_root)

    inst_files = _discover_instances(inst_dir)

    # è‹¥æŒ‡å®šäº†é¡ºåºæ–‡ä»¶ï¼Œåˆ™è¿›è¡Œè¿‡æ»¤å’Œæ’åº
    if args.order_file:
        inst_files = _apply_order_file(inst_files, _ensure_abs(args.order_file, project_root))

    if args.limit is not None:
        inst_files = inst_files[: max(0, int(args.limit))]

    if not inst_files:
        print(f"æœªåœ¨ {inst_dir} æ‰¾åˆ°å®ä¾‹ JSON æ–‡ä»¶")
        sys.exit(1)

    import uuid

    # ä¸´æ—¶ç©ºé—´ï¼štmp/instance_runner/<timestamp-like>_<random_suffix>
    # æ·»åŠ éšæœºåç¼€é˜²æ­¢å¤šæ¬¡è¿è¡Œæ—¶ç›®å½•å†²çª
    random_suffix = str(uuid.uuid4())[:8]
    tmp_root = project_root / "tmp" / "instance_runner" / f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random_suffix}"
    tmp_root.mkdir(parents=True, exist_ok=True)

    mapping = _prepare_temp_space(inst_files, tmp_root)

    # ç”Ÿæˆ timestamp å¹¶è®¡ç®—è¾“å‡ºæ ¹ç›®å½•ï¼ˆæ”¯æŒ CLI è¦†ç›–ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if args.output_dir:
        # å¦‚æœä¼ å…¥çš„è¦†ç›–ç›®å½•åŒ…å«å ä½ç¬¦ï¼Œåˆ™æ›¿æ¢ï¼›å¦åˆ™æŒ‰åŸæ ·ä½¿ç”¨
        override_root = str(_ensure_abs(args.output_dir, project_root))
        base_root_dir = override_root.replace("{timestamp}", timestamp)
    else:
        base_root_dir = base_cfg.get("output_dir", "trajectories_perf/run_{timestamp}").replace(
            "{timestamp}", timestamp
        )

    # æ„å»º per-instance é…ç½®æ–‡ä»¶å¹¶è¿è¡Œ
    per_instance_cfg_paths: dict[str, Path] = {}
    for name, tmp_inst_dir in mapping.items():
        cfg_out = tmp_inst_dir / "se_config.yaml"
        per_instance_cfg_paths[name] = _write_per_instance_config(
            base_cfg,
            tmp_inst_dir,
            name,
            cfg_out,
            timestamp,
            override_base_out=base_root_dir,
        )

    # å¹¶è¡Œæ‰§è¡Œï¼ˆç®€å•çš„æ‰¹æ¬¡è°ƒåº¦ï¼šçª—å£å¤§å° = max_parallelï¼‰
    names = list(per_instance_cfg_paths.keys())
    total = len(names)
    max_parallel = max(1, int(args.max_parallel))
    i = 0
    successes = 0
    failures = 0
    # print(f"å‡†å¤‡è¿è¡Œ {total} ä¸ªå®ä¾‹ï¼›å¹¶è¡Œåº¦ = {max_parallel}ï¼›æ¨¡å¼ = {args.mode}ï¼›dry_run = {args.dry_run}")

    # å‡†å¤‡ä¸€ä¸ªå…¨å±€åˆ—è¡¨æ¥è·Ÿè¸ªæ­£åœ¨è¿è¡Œçš„å­è¿›ç¨‹
    active_processes: list[subprocess.Popen] = []
    active_processes_lock = threading.Lock()
    stop_event = threading.Event()
    exec_ctx = {"executor": None, "futures": []}

    def _cleanup_processes():
        with active_processes_lock:
            procs = list(active_processes)

        if not procs:
            return

        print(f"\næ­£åœ¨ç»ˆæ­¢ {len(procs)} ä¸ªæ´»è·ƒå­è¿›ç¨‹...")
        for p in procs:
            try:
                if p.poll() is None:
                    try:
                        os.killpg(p.pid, signal.SIGTERM)
                    except Exception:
                        pass
                    try:
                        p.terminate()
                    except Exception:
                        pass
            except Exception:
                pass

        deadline = time.time() + 5
        for p in procs:
            try:
                while p.poll() is None and time.time() < deadline:
                    time.sleep(0.1)
                if p.poll() is None:
                    try:
                        os.killpg(p.pid, signal.SIGKILL)
                    except Exception:
                        pass
                    try:
                        p.kill()
                    except Exception:
                        pass
            except Exception:
                pass

    def _signal_handler(sig, frame):
        print(f"\næ¥æ”¶åˆ°ä¿¡å· {sig}ï¼Œå‡†å¤‡é€€å‡º...")
        stop_event.set()
        # å°è¯•å–æ¶ˆæœªå¼€å§‹çš„ä»»åŠ¡
        try:
            if exec_ctx["executor"] is not None:
                exec_ctx["executor"].shutdown(wait=False, cancel_futures=True)
        except Exception:
            pass
        # ç»ˆæ­¢æ‰€æœ‰å·²å¯åŠ¨çš„å­è¿›ç¨‹
        _cleanup_processes()

    # æ³¨å†Œä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, _signal_handler)
    signal.signal(signal.SIGTERM, _signal_handler)

    # åŒ…è£… subprocess.run ä»¥ä¾¿è·Ÿè¸ª Popen å¯¹è±¡
    def _run_process(name, cmd, cwd):
        with stats_lock:
            stats["started"] += 1
            stats["running"] += 1
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{now_str}] å¼€å§‹ä»»åŠ¡: {name}")

        try:
            # ä½¿ç”¨ DEVNULL æŠ‘åˆ¶è¾“å‡ºï¼Œé™¤é debug éœ€è¦
            with subprocess.Popen(
                cmd, cwd=cwd, text=True, start_new_session=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
            ) as proc:
                # with subprocess.Popen(
                #     cmd, cwd=cwd, text=True, start_new_session=True
                # ) as proc:
                with active_processes_lock:
                    active_processes.append(proc)

                try:
                    proc.wait()
                    return subprocess.CompletedProcess(proc.args, proc.returncode)
                finally:
                    with active_processes_lock:
                        if proc in active_processes:
                            active_processes.remove(proc)
        except Exception as e:
            raise e
        finally:
            with stats_lock:
                stats["running"] -= 1
                stats["completed"] += 1

    if args.dry_run:
        for name in names:
            cfg_path = per_instance_cfg_paths[name]
            cmd = _build_perf_cmd(cfg_path, args.mode, project_root)
            print(f"[DRY-RUN] {name}: {' '.join(cmd)}")
            successes += 1
    else:
        from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED

        # åˆå§‹åŒ–ç»Ÿè®¡æ•°æ®
        stats = {
            "started": 0,
            "running": 0,
            "completed": 0,
        }
        stats_lock = threading.Lock()

        # åˆå§‹æ‰“å°ä¸€æ¬¡
        def _print_status():
            now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            with stats_lock:
                run = stats["running"]
                start = stats["started"]
                comp = stats["completed"]
            # Remaining = Total - Started
            rem = total - start
            print(
                f"[{now_str}] STATUS: Running {run}/{max_parallel} | Started {start} | Completed {comp} | Remaining {rem}"
            )

        _print_status()
        last_print_time = time.time()

        with ThreadPoolExecutor(max_workers=max_parallel) as executor:
            exec_ctx["executor"] = executor
            future_map: dict[concurrent.futures.Future, str] = {}

            def _submit_by_index(idx: int):
                n = names[idx]
                cfg_path = per_instance_cfg_paths[n]
                c = _build_perf_cmd(cfg_path, args.mode, project_root)
                f = executor.submit(_run_process, n, c, cwd=str(project_root))
                future_map[f] = n
                exec_ctx["futures"].append(f)
                return f

            next_idx = 0
            initial = min(max_parallel, total)
            for _ in range(initial):
                if stop_event.is_set():
                    break
                _submit_by_index(next_idx)
                next_idx += 1

            remaining = set(future_map.keys())
            while remaining:
                if stop_event.is_set():
                    break

                now = time.time()
                if now - last_print_time >= 15:
                    _print_status()
                    last_print_time = now

                done, not_done = wait(remaining, timeout=1.0, return_when=FIRST_COMPLETED)
                new_futs = []
                for fut in done:
                    n = future_map.get(fut, "unknown")
                    try:
                        res = fut.result()
                        rc = getattr(res, "returncode", None)
                        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        print(f"[{now_str}] å®Œæˆä»»åŠ¡: {n} (rc={rc})")
                        if rc == 0:
                            successes += 1
                        else:
                            failures += 1
                    except Exception:
                        failures += 1
                        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        print(f"[{now_str}] å®Œæˆä»»åŠ¡: {n} (å¼‚å¸¸)")

                    if not stop_event.is_set() and next_idx < total:
                        next_name = names[next_idx]
                        nf = _submit_by_index(next_idx)
                        new_futs.append(nf)
                        next_idx += 1
                        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        print(f"[{now_str}] å¯åŠ¨æ–°ä»»åŠ¡: {next_name}")

                remaining = not_done.union(set(new_futs))

            # Final status
            _print_status()

            # å¦‚æœæ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œå°è¯•å–æ¶ˆå‰©ä½™ä»»åŠ¡
            if stop_event.is_set():
                for fut in remaining:
                    try:
                        fut.cancel()
                    except Exception:
                        pass
                _cleanup_processes()
                print("å·²ç»ˆæ­¢è¿è¡Œï¼Œæ­£åœ¨æ¸…ç†èµ„æº...")
                # ç»ˆæ­¢æµç¨‹åä¸å†è¿›è¡Œåç»­èšåˆä¸ç»Ÿè®¡
                return

    print(f"è¿è¡Œç»“æŸï¼šæˆåŠŸ {successes}/{total}ï¼›å¤±è´¥ {failures}")

    if not args.dry_run:
        try:
            with contextlib.redirect_stdout(open(os.devnull, "w")):
                res = collect_outputs(base_root_dir, names)
            print(f"èšåˆ traj.pool: {res.get('traj_pool')}")
            print(f"èšåˆ all_hist.json: {res.get('all_hist')}")
            print(f"èšåˆ final.json: {res.get('final')}")
        except Exception:
            print("æ‰¹æ¬¡èšåˆå¤±è´¥")

        # ç»Ÿè®¡æ‰€æœ‰å®ä¾‹çš„ Token æ¶ˆè€—
        try:
            # per_instance_cfg_paths é‡Œçš„ key æ˜¯ instance_nameï¼Œä½†æˆ‘ä»¬éœ€è¦çš„æ˜¯å®é™…çš„è¾“å‡ºç›®å½•
            # åœ¨ main å‡½æ•°å‰é¢æˆ‘ä»¬è®¡ç®—äº†: final_base_out = orig_out.replace("{timestamp}", timestamp)
            # å¹¶ä¸” per instance çš„ output_dir æ˜¯ f"{final_base_out}/{instance_name}"
            # æ‰€ä»¥æˆ‘ä»¬å¯ä»¥é‡å»ºè¿™äº›è·¯å¾„

            instance_output_dirs = []
            for name in names:
                inst_out_dir = Path(base_root_dir) / name
                instance_output_dirs.append(inst_out_dir)

            _aggregate_token_stats(instance_output_dirs, base_root_dir)
        except Exception as e:
            print(f"Token èšåˆç»Ÿè®¡å¤±è´¥: {e}")


if __name__ == "__main__":
    main()
